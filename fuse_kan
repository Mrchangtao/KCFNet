import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
import pdb
from torch.autograd.variable import Variable
from torch.nn.modules.loss import _Loss
from einops import rearrange

from .kan import *


class DW_bn_relu(nn.Module):
    def __init__(self, dim=768):
        super(DW_bn_relu, self).__init__()
        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)
        self.bn = nn.BatchNorm2d(dim)
        self.silu = nn.SiLU()

    def forward(self, x, H, W):
        B, N, C = x.shape
        x = x.transpose(1, 2).view(B, C, H, W)
        x = self.dwconv(x)
        x = self.bn(x)
        x = self.silu(x)
        x = x.flatten(2).transpose(1, 2)

        return x

class KANLayer(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.dim = in_features

        grid_size = 5
        spline_order = 3
        scale_noise = 0.1
        scale_base = 1.0
        scale_spline = 1.0
        base_activation = torch.nn.SiLU
        grid_eps = 0.02
        grid_range = [-1, 1]

        self.fc1 = KANLinear(
                in_features,  # 160
                hidden_features,  # 160
                grid_size=grid_size,  # 5
                spline_order=spline_order,  # 3
                scale_noise=scale_noise,  # 0.1
                scale_base=scale_base,  # 1.0
                scale_spline=scale_spline,  # 1.0
                base_activation=base_activation,  # SiLu
                grid_eps=grid_eps,  # 0.02
                grid_range=grid_range,  # [-1,1]
            )
#         # self.fc2 = KANLinear(
#         #         hidden_features,  # 160
#         #         out_features,  # 160
#         #         grid_size=grid_size,  # 5
#         #         spline_order=spline_order,  # 3
#         #         scale_noise=scale_noise,  # 0.1
#         #         scale_base=scale_base,  # 1
#         #         scale_spline=scale_spline,  # 1
#         #         base_activation=base_activation,
#         #         grid_eps=grid_eps,
#         #         grid_range=grid_range,
#         #     )




        self.dwconv_1 = DW_bn_relu(hidden_features)   # 深度卷积 + 归一化 + Relu激活
#         # self.dwconv_2 = DW_bn_relu(hidden_features)
#         # self.dwconv_3 = DW_bn_relu(hidden_features)
#         # # # TODO
#         # self.dwconv_4 = DW_bn_relu(hidden_features)
#         # self.dwconv_5 = DW_bn_relu(hidden_features)

        self.drop = nn.Dropout(drop)  # 正则化

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            fan_out //= m.groups
            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
            if m.bias is not None:
                m.bias.data.zero_()

    def forward(self, x, H, W):
        # pdb.set_trace()
        B, N, C = x.shape

        x = self.fc1(x.reshape(B*N,C))
        x = x.reshape(B,N,C).contiguous()
        x = self.dwconv_1(x, H, W)
#         # x = self.fc2(x.reshape(B*N,C))
#         # x = x.reshape(B,N,C).contiguous()
#         # x = self.dwconv_2(x, H, W)

        

#         # # TODO
#         # x = x.reshape(B,N,C).contiguous()
#         # x = self.dwconv_4(x, H, W)
    
        return x

class KANBlock(nn.Module):
    def __init__(self, dim, drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, no_kan=False):
        super().__init__()

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim)

        self.layer = KANLayer(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            fan_out //= m.groups
            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
            if m.bias is not None:
                m.bias.data.zero_()

    def forward(self, x, H, W):
        x = x + self.drop_path(self.layer(self.norm2(x), H, W))

        return x


class PrimaryCaps_semantic(nn.Module):
    def __init__(self, c1=32, c2=8, K=1, P=4, stride=1):
        super(PrimaryCaps_semantic, self).__init__()
        self.pose = nn.Conv2d(in_channels=c1, out_channels=c2*P*P,
                            kernel_size=K, stride=stride, padding=1, bias=True)
        self.a = nn.Conv2d(in_channels=c1, out_channels=c2,
                            kernel_size=K, stride=stride, padding=1, bias=True)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):                 # [b,256,24,24]
        p = self.pose(x)                  #【b,8*16,24,24】   
        a = self.a(x)                     # [b,8*1,24,24]
        a = self.sigmoid(a)
        out = torch.cat([p, a], dim=1)     #[b,136,24,24]
        return out, p, a

class ConvCaps_semantic(nn.Module):
    def __init__(self, B=8, C=8, K=1, P=4, stride=2, iters=3,
                 coor_add=False, w_shared=False, horizontal=False, vertical=False):
        super(ConvCaps_semantic, self).__init__()
        # TODO: lambda scheduler
        # Note that .contiguous() for 3+ dimensional tensors is very slow
        self.horizontal = horizontal
        self.vertical = vertical
        self.B = B
        self.C = C
        self.K = K
        self.P = P
        self.psize = P*P
        self.stride = stride
        self.iters = iters
        self.coor_add = coor_add
        self.w_shared = w_shared
        # constant
        self.eps = 1e-8
        self._lambda = 1e-03
        self.ln_2pi = torch.FloatTensor(1).fill_(math.log(2*math.pi))    #删去cuda！！！
        # params
        # Note that \beta_u and \beta_a are per capsule type,
        # which are stated at https://openreview.net/forum?id=HJWLfGWRb&noteId=rJUY2VdbM
        self.beta_u = nn.Parameter(torch.zeros(C))
        self.beta_a = nn.Parameter(torch.zeros(C))
        # Note that the total number of trainable parameters between
        # two convolutional capsule layer types is 4*4*k*k
        # and for the whole layer is 4*4*k*k*B*C,
        # which are stated at https://openreview.net/forum?id=HJWLfGWRb&noteId=r17t2UIgf
        self.weights = nn.Parameter(torch.randn(1, K*K*B, C, P, P))
        # op
        self.sigmoid = nn.Sigmoid()
        self.softmax = nn.Softmax(dim=2)

    def m_step(self, a_in, r, v, eps, b, B, C, psize):

        r = r * a_in
        r = r / (r.sum(dim=2, keepdim=True) + eps)
        r_sum = r.sum(dim=1, keepdim=True)
        coeff = r / (r_sum + eps)
        coeff = coeff.view(b, B, C, 1)

        mu = torch.sum(coeff * v, dim=1, keepdim=True)
        sigma_sq = torch.sum(coeff * (v - mu)**2, dim=1, keepdim=True) + eps

        r_sum = r_sum.view(b, C, 1)
        sigma_sq = sigma_sq.view(b, C, psize)
        cost_h = (self.beta_u.view(C, 1) + torch.log(sigma_sq.sqrt())) * r_sum

        a_out = self.sigmoid(self._lambda*(self.beta_a - cost_h.sum(dim=2)))
        sigma_sq = sigma_sq.view(b, 1, C, psize)

        return a_out, mu, sigma_sq

    def e_step(self, mu, sigma_sq, a_out, v, eps, b, C):
        ln_p_j_h = -1. * (v - mu)**2 / (2 * sigma_sq) \
                    - torch.log(sigma_sq.sqrt()) \
                    - 0.5*self.ln_2pi

        ln_ap = ln_p_j_h.sum(dim=3) + torch.log(a_out.view(b, 1, C))
        r = self.softmax(ln_ap)
        return r

    def caps_em_routing(self, v, a_in, C, eps):
        b, B, c, psize = v.shape        #[1,8,8,16]
        assert c == C
        assert (b, B, 1) == a_in.shape     #[1,8,1]

        r = torch.FloatTensor(b, B, C).fill_(1./C)   #删去了cuda

        #保持变量cuda一致
        if a_in.is_cuda and a_in.get_device() == 0:
            device = torch.device("cuda:0")
            r=r.to(device)
            self.ln_2pi=self.ln_2pi.to(device)   #维护变量之间是一致的device类型
        elif a_in.is_cuda and a_in.get_device() == 1:
            device = torch.device("cuda:1")
            r=r.to(device)
            self.ln_2pi=self.ln_2pi.to(device)   #维护变量之间是一致的device类型
        elif a_in.is_cuda and a_in.get_device() == 2:
            device = torch.device("cuda:2")
            r=r.to(device)
            self.ln_2pi=self.ln_2pi.to(device)   #维护变量之间是一致的device类型
        elif a_in.is_cuda and a_in.get_device() == 3:
            device = torch.device("cuda:3")
            r=r.to(device)
            self.ln_2pi=self.ln_2pi.to(device)   #维护变量之间是一致的device类型

        for iter_ in range(self.iters):
            a_out, mu, sigma_sq = self.m_step(a_in, r, v, eps, b, B, C, psize)
            if iter_ < self.iters - 1:
                r = self.e_step(mu, sigma_sq, a_out, v, eps, b, C)

        return mu, a_out

    def add_pathes(self, x, B, K, psize, stride):
        """
            Shape:
                Input:     (b, H, W, B*(P*P+1))
                Output:    (b, H', W', K, K, B*(P*P+1))
        """
        b, h, w, c = x.shape       #[1,1,1,136]
        assert h == w         
        assert c == B*(psize+1)   
        oh = ow = 1 
        x = torch.unsqueeze(x, dim=1)
        x = torch.unsqueeze(x, dim=3)
        x = x.permute(0, 1, 3, 2, 4, 5).contiguous()      #[1,oh=1,ow=1,1,1,136]
        return x, oh, ow

    def transform_view(self, x, w, C, P, w_shared=False):
        """
            For conv_caps:
                Input:     (b*H*W, K*K*B, P*P)
                Output:    (b*H*W, K*K*B, C, P*P)
            For class_caps:
                Input:     (b, H*W*B, P*P)
                Output:    (b, H*W*B, C, P*P)
        """
        b, B, psize = x.shape             #[1*1*1=1,1*1*8=8,16]
        assert psize == P*P

        x = x.view(b, B, 1, P, P)         #[1,8,1,4,4]
        if w_shared:
            hw = int(B / w.size(1))
            w = w.repeat(1, hw, 1, 1, 1)

        w = w.repeat(b, 1, 1, 1, 1)        #w原来是[1, K*K*B=1*1*8, C=8, P=4, P=4]现在也是因为b=batch-size*oh*ow
        x = x.repeat(1, 1, C, 1, 1)        #[1,8,8,4,4]
        v = torch.matmul(x, w)             #[1,8,8,4,4]
        v = v.view(b, B, C, P*P)           #[1,8,8,16]
        return v        

    def add_coord(self, v, b, h, w, B, C, psize):
        """
            Shape:
                Input:     (b, H*W*B, C, P*P)
                Output:    (b, H*W*B, C, P*P)
        """
        assert h == w
        v = v.view(b, h, w, B, C, psize)
        coor = torch.arange(h, dtype=torch.float32) / h
        coor_h = torch.cuda.FloatTensor(1, h, 1, 1, 1, self.psize).fill_(0.)
        coor_w = torch.cuda.FloatTensor(1, 1, w, 1, 1, self.psize).fill_(0.)
        coor_h[0, :, 0, 0, 0, 0] = coor
        coor_w[0, 0, :, 0, 0, 1] = coor
        v = v + coor_h + coor_w
        v = v.view(b, h*w*B, C, psize)
        return v

    def forward(self, x):         #[1,136,1,1]
        x=x.permute(0, 2, 3, 1)   #为了和前面那一层维度对应  
        b, h, w, c = x.shape      #[1,1,1,8*17]          
        if not self.w_shared:
            # add patches
            x, oh, ow = self.add_pathes(x, self.B, self.K, self.psize, self.stride)

            # transform view
            p_in = x[:, :, :, :, :, :self.B*self.psize].contiguous()
            a_in = x[:, :, :, :, :, self.B*self.psize:].contiguous()        #分离p和a
            p_in = p_in.view(b*oh*ow, self.K*self.K*self.B, self.psize)     #[1,8,16]
            a_in = a_in.view(b*oh*ow, self.K*self.K*self.B, 1)              #[1,8,1]
            v = self.transform_view(p_in, self.weights, self.C, self.P)     #[1,8,8,16]

            v = self.add_coord(v, b, h, w, self.B, self.C, self.psize)

            # em_routing  p:prompt   
            p_out, a_out = self.caps_em_routing(v, a_in, self.C, self.eps)  #[b,1,8,16]  [b,8]
            p_out_p = torch.sum(p_out, dim=1)    #[b,8,16]
            a_out_p = torch.unsqueeze(a_out,dim=2)  
            out_p = torch.cat([p_out_p, a_out_p], dim=2)  


            # p_out = p_out.view(b, oh, ow, self.C*self.psize)
            p_out = p_out.view(b, oh, ow, self.C*self.psize)               #[1,1,1,128]
            a_out = a_out.view(b, oh, ow, self.C)                          #[1,1,1,8]
            out = torch.cat([p_out, a_out], dim=3)                         #[1,1,1,136]
            out = out.permute(0, 3, 1, 2)   #输出yolo可以接受的维度顺序      #[1,136,1,1]
            a_out = a_out.permute(0, 3, 1, 2)
        else:
            assert c == self.B*(self.psize+1)
            assert 1 == self.K
            assert 1 == self.stride
            p_in = x[:, :, :, :self.B*self.psize].contiguous()
            p_in = p_in.view(b, h*w*self.B, self.psize)
            a_in = x[:, :, :, self.B*self.psize:].contiguous()
            a_in = a_in.view(b, h*w*self.B, 1)

            # transform view
            v = self.transform_view(p_in, self.weights, self.C, self.P, self.w_shared)

            # coor_add
            if self.coor_add:
                v = self.add_coord(v, b, h, w, self.B, self.C, self.psize)

            # em_routing
            _, out = self.caps_em_routing(v, a_in, self.C, self.eps)
            _, out = _, out.permute(0, 3, 1, 2)  #输出yolo可以接受的维度顺序

        return p_out_p,a_out_p,out_p ,out
    
def autopad(k, p=None, d=1):  # kernel, padding, dilation
    # Pad to 'same' shape outputs
    if d > 1:
        k = d * (k - 1) + 1 if isinstance(k, int) else [d * (x - 1) + 1 for x in k]  # actual kernel-size
    if p is None:
        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad
    return p
    
class ReLUConv(nn.Module):
    # Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)
    default_act = nn.ReLU()  # default activation

    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):
        super().__init__()
        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)
        self.bn = nn.BatchNorm2d(c2)
        self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()

    def forward(self, x):
        return self.act(self.bn(self.conv(x)))

    def forward_fuse(self, x):
        return self.act(self.conv(x))




# --- 通道注意力 (CA) ---
class ChannelAttention(nn.Module):
    def __init__(self, in_channels, ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.fc1 = nn.Conv2d(in_channels, in_channels // ratio, 1, bias=False)
        self.relu1 = nn.ReLU()
        self.fc2 = nn.Conv2d(in_channels // ratio, in_channels, 1, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        out = avg_out + max_out
        return self.sigmoid(out)

# --- 空间注意力 (SA) ---
class SpatialAttention(nn.Module):
    def __init__(self):
        super(SpatialAttention, self).__init__()
        self.conv1 = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)

class Semantics_kan(nn.Module):
    def __init__(self, A=1024, B=32, C=32, K=3, P=4, iters=3, b=1, w=12, h=12, out_channel=1024):
        super(Semantics_kan, self).__init__()
        self.A = A
        self.B = B
        self.C = C
        self.K = K
        self.P = P
        self.b = b
        self.w = w
        self.h = h

        self.primary_caps_ = PrimaryCaps_semantic(c1=A, c2=B, K=3, P=P, stride=1)
        self.norm1 = nn.LayerNorm(B * (P * P))
        self.norm2 = nn.LayerNorm(B)

        # --- KAN ---
        self.kanlayer_pose = KANBlock(dim=512)   #使用 512
        self.kanlayer_act = KANBlock(dim=32)

        self.mlp1 = nn.Sequential(
            nn.Linear(B * (P * P ) , B * (P * P )),
            nn.GELU(),
            nn.Linear(B * (P * P ), B * (P * P )),
        )

        # --- 注意力模块 ---
        self.ca_act = ChannelAttention(in_channels=B)       # 用于 Act 的 CA
        self.sa_act = SpatialAttention()                    # 用于 Act 的 SA
        self.ca_pose = ChannelAttention(in_channels=B * (P * P))  # 用于 Pose 的 CA
        self.sa_pose = SpatialAttention()                    # 用于 Pose 的 SA
        self.conv1x1 = nn.Conv2d(B, B * (P*P), kernel_size=1)

        self.weighted_act = None
        self.weighted_pose = None
        self.kan_pose_input = None
        self.kan_act_input = None

    def forward(self, x):
        caps, pose, act = self.primary_caps_(x)

        # --- 维度变换 ---
        cut_1 = [[(h_idx + k_idx) for k_idx in range(0, self.B)] for h_idx in range(0, self.P * self.P * self.B, self.B)]
        cut_2 = [[(h_idx + k_idx) for k_idx in range(0, self.B)] for h_idx in range(0, 1 * self.B, self.B)]
        pose_ = pose[:, cut_1, :, :].contiguous()
        act_ = act[:, cut_2, :, :].contiguous()
        Initial_pose = rearrange(pose_ , " d e f g h->d g h (e f)")     # [, , , ]
        # print("Initial_pose_1.shape:", Initial_pose.shape)
        Initial_act = rearrange(act_ , " d e f g h->d g h (e f)")

        Initial_pose_norm = self.norm1(Initial_pose)  # [B, H, W, 512]
        Initial_act_norm = self.norm2(Initial_act)   # [B, H, W, 32]

        # --- Act 流程 ---
        # 1. CA
        act_ca = self.ca_act(Initial_act_norm.permute(0, 3, 1, 2))  # [B, 32, H, W]
        act_ca = act_ca * Initial_act_norm.permute(0, 3, 1, 2)


        # 2. SA
        act_sa = self.sa_act(act_ca)  # [B, 1, H, W]
        self.weighted_act = act_sa * act_ca    # [B, 32, H, W]

        # --- Pose 流程 ---
        # 1. CA
        pose_ca = self.ca_pose(Initial_pose_norm.permute(0, 3, 1, 2))  # [B, 512, H, W]
        pose_ca = pose_ca * Initial_pose_norm.permute(0, 3, 1, 2)   # [B, 512, H, W]

        # 2. SA
        pose_sa = self.sa_pose(pose_ca)  # [B, 1, H, W]
        self.weighted_pose = pose_sa * pose_ca  #[B, 512, H, W]

         # --- act 和 pose 加权求和 ---

        weighted_act_ = self.conv1x1(self.weighted_act)                      # [B, 512, H, W]
        fused = self.weighted_pose + weighted_act_                         # [B, 512, H, W]

  

        # --- KAN (分别处理 weighted_pose 和 weighted_act) ---   
        B, C, H, W = self.weighted_pose.shape        
        self.kan_pose_input = fused.reshape(B, -1, C)
        kan_pose_output = self.kanlayer_pose(self.kan_pose_input, H, W)    
        kan_pose_output = kan_pose_output.reshape(B, H, W, C)         
        kan_pose_output = kan_pose_output.permute(0, 3, 1, 2)            

        B, C, H, W = self.weighted_act.shape                                     
        self.kan_act_input = self.weighted_act.reshape(B, -1, C)                  
        kan_act_output = self.kanlayer_act(self.kan_act_input, H, W)       
        kan_act_output = kan_act_output.reshape(B, H, W, C)    
        kan_act_output = kan_act_output.permute(0, 3, 1, 2)                 


        return  kan_act_output  , kan_pose_output

